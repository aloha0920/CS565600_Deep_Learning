{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Sentiment Analysis & Neural Machine Translation</center>\n",
    "<center>Shan-Hung Wu & DataLab<br/>Fall 2017</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we want to show how to use **recurrent neural networks** (**rnn**) to model continuous sequence like nature language, and use it on not only article comprehension but also word generation. We will introduce two datasets, [Large Movie Review](http://ai.stanford.edu/~amaas/data/sentiment/) for sentiment analysis and [UM-Corpus](http://www.lrec-conf.org/proceedings/lrec2014/pdf/774_Paper.pdf) for machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Recurrent Neural Networks</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='imgs/recurrent_neural_networks.jpg' width='60%' /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several RNN related layers already implemented in [tensorflow.contrib.rnn](https://www.tensorflow.org/api_guides/python/contrib.rnn). To construct a **simple rnn**, we should first define the **cell** and apply it into the network. Here is a dummy example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mosquito/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"rnn/transpose:0\", shape=(4, 12, 128), dtype=float32)\n",
      "LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_2:0' shape=(4, 128) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_3:0' shape=(4, 128) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "simple_cell = tf.contrib.rnn.BasicRNNCell(num_units=128)\n",
    "lstm_cell = tf.contrib.rnn.LSTMCell(num_units=128) # you can also use advanced LSTM or GRU cell\n",
    "outputs, state = tf.nn.dynamic_rnn(lstm_cell, tf.constant(np.float32(np.random.rand(4, 12, 300))), dtype=tf.float32)\n",
    "\n",
    "print(outputs)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Sentiment Analysis</center>\n",
    "We try to do sentiment analysis using RNN for **modeling a movie review** and judege it's **a favorable comment or not**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0  remarkable fact participation klaus kinski pla...          0\n",
      "1  bought movie dvd watched first time night fan ...          1\n",
      "2  movie proves judge movie awesome artwork dvd c...          0\n",
      "3  gave film superbly consistent movie pure abili...          1\n",
      "4  hm enjoyable movie poke plot holes point atroc...          0\n",
      "5  watching movie waste time tempted leave middle...          0\n",
      "6  engrossing drama four men canoing weekend remo...          1\n",
      "7  bad acting bad writing poorly written film bad...          0\n",
      "8  innocent man steve guttenberg one night stand ...          0\n",
      "9  broken bow takes us back began set years futur...          1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_csv = pd.read_csv('dataset/imdb/train.csv')\n",
    "test_csv = pd.read_csv('dataset/imdb/test.csv')\n",
    "\n",
    "print(train_csv.head(n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To feed a word into RNN one by one, we should **transform words into vectors** first which is [Word2vec](https://en.wikipedia.org/wiki/Word2vec) we have discussed in previous lab. Instead of train ourself, we use an **existing Word2vec** by [spaCy](https://spacy.io/) this time. For those who first use spaCy, you can install it via pip and don't forget to **download English model**.\n",
    "```\n",
    "    pip install spacy\n",
    "    python -m spacy download en\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple vs banana: 0.635210\n",
      "apple vs mac: 0.555320\n",
      "banana vs mac: 0.636669\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "word_apple = nlp('apple')\n",
    "word_banana = nlp('banana')\n",
    "word_mac = nlp('mac')\n",
    "\n",
    "print('%s vs %s: %.6f'%(word_apple, word_banana, word_apple.similarity(word_banana)))\n",
    "print('%s vs %s: %.6f'%(word_apple, word_mac, word_apple.similarity(word_mac)))\n",
    "print('%s vs %s: %.6f'%(word_banana, word_mac, word_banana.similarity(word_mac)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like [Competition01](https://nthu-datalab.github.io/ml/competitions/01_Response_Selection/01_Response_Selection.html), we should do some text preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['remarkable fact participation plays priest ask bad bad movie overall', 0], ['bought movie watched first time night fan work years ever since got high school grow movie perhaps mother kept away definitely children bad graphic sense themes would go right year olds heads overall animation excellent considering made thought story followed book fairly well loose sense bothered add another minutes tack return king parts would made ultimate movie bothered abrupt end heard return king sucked bothered even bad one great animation film might garnered higher vote give hope books justice new live action', 1], ['movie proves judge movie awesome artwork cover also goes show learn movie buy get someone beginning movie actually looks somewhat promising well meet characters pumpkin jack old guy street brings college co book full witch spells leaves annual haunted house movie takes place drinking fighting soft core porn action movie finally takes place hour overall end predictable reminiscent soft core porn movie probably best viewed group friends nothing better good movie make fun first time viewers really fun making predictions order people die', 0]]\n"
     ]
    }
   ],
   "source": [
    "def remove_unknown_words(csv):\n",
    "    dat = []\n",
    "    n = len(csv)\n",
    "    \n",
    "    for i in range(n):\n",
    "        s = csv.loc[i]['review']\n",
    "        s = ' '.join([w for w in s.split(' ') if w in nlp.vocab])\n",
    "\n",
    "        dat.append([s, csv.loc[i]['sentiment']])\n",
    "    \n",
    "    return dat\n",
    "\n",
    "train_dat = remove_unknown_words(train_csv)\n",
    "test_dat = remove_unknown_words(test_csv)\n",
    "\n",
    "print(train_dat[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding and Bucketing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='imgs/padding_and_bucketing.png' width='60%' /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For batch training, we should make sure that the **sequences in smae batch have same length**. In nature language processing, we are used to **adding some token** after finished sentence which we means **padding**. But if there are too many paddings token in batch, it may hurt model's performance since **padding will domain whole sentnence** and your model can learn nothing. We introduce a naive solution to this, bucketing. Bucketing is to **re-arrange your sentences** so that each batch has **as similar length as possible**, preventing lots of paddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGenerator:\n",
    "    def __init__(self, dat, batch_size):\n",
    "        n = len(dat)\n",
    "        lst = [i for i in range(n)]\n",
    "        lst = sorted(lst, key=lambda i: len(dat[i][0].split(' ')))\n",
    "        \n",
    "        self.batch_xs, self.batch_ys, self.reviews = [], [], []\n",
    "        \n",
    "        for i in range(n//batch_size):\n",
    "            long = len(dat[lst[(i+1)*batch_size-1]][0].split(' '))\n",
    "            batch_x = np.zeros((batch_size, long, 300))\n",
    "            batch_y = np.zeros((batch_size, 2))\n",
    "            review = []\n",
    "            \n",
    "            for j in range(batch_size):\n",
    "                words = dat[lst[i*batch_size+j]][0].split(' ')\n",
    "                for k in range(len(words)):\n",
    "                    batch_x[j][k] = nlp(words[k]).vector # use existing Word2vec model\n",
    "                for k in range(k, long):\n",
    "                    batch_x[j][k] = nlp(' ').vector # padding with ' '\n",
    "                \n",
    "                batch_y[j][dat[lst[i*batch_size+j]][1]] = 1 # represent class as 1-hot vector\n",
    "                review.append(dat[lst[i*batch_size+j]][0])\n",
    "            \n",
    "            self.batch_xs.append(batch_x)\n",
    "            self.batch_ys.append(batch_y)\n",
    "            self.reviews.append(review)\n",
    "        \n",
    "    def get(self, batch_id):\n",
    "        return self.batch_xs[batch_id], self.batch_ys[batch_id], self.reviews[batch_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (384) into shape (300)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ddd2899eb560>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-209340e029b9>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dat, batch_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                     \u001b[0mbatch_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector\u001b[0m \u001b[0;31m# use existing Word2vec model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                     \u001b[0mbatch_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector\u001b[0m \u001b[0;31m# padding with ' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (384) into shape (300)"
     ]
    }
   ],
   "source": [
    "batch = BatchGenerator(train_dat, 32)\n",
    "\n",
    "xs, ys, rv = batch.get(0)\n",
    "print(xs[0])\n",
    "print(ys[0])\n",
    "print(rv[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building RNN Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start to build our RNN model. **Adding clear variable scope** for each component in the graph is a good habbit, and it may help much when cooperating with aother one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SentimentReviewRNN:\n",
    "    def __init__(self):\n",
    "        with tf.variable_scope('rnn_i/o'):\n",
    "            # use None for batch size and dynamic sequence length\n",
    "            self.inputs = tf.placeholder(tf.float32, shape=[None, None, 300])\n",
    "            self.groundtruths = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "        \n",
    "        with tf.variable_scope('rnn_cell'):\n",
    "            self.cell = tf.contrib.rnn.LSTMCell(128)\n",
    "            # project RNN output into target class dimension\n",
    "            self.out_cell = tf.contrib.rnn.OutputProjectionWrapper(self.cell, 2)\n",
    "        \n",
    "        with tf.variable_scope('rnn_forward'):\n",
    "            # use dynamic_rnn for different length\n",
    "            self.outputs, _ = tf.nn.dynamic_rnn(self.out_cell, self.inputs, dtype=tf.float32) \n",
    "            self.outputs = self.outputs[:, -1, :] # only use the last output of sequence\n",
    "        \n",
    "        with tf.variable_scope('rnn_loss'):\n",
    "            # use cross_entropy as class loss\n",
    "            self.loss = tf.losses.softmax_cross_entropy(onehot_labels=self.groundtruths, logits=self.outputs)\n",
    "            self.optimizer = tf.train.AdamOptimizer(0.02).minimize(self.loss)\n",
    "        \n",
    "        with tf.variable_scope('rnn_accuracy'):\n",
    "            self.accuracy = tf.contrib.metrics.accuracy(labels=tf.argmax(self.groundtruths, axis=1), \n",
    "                                                        predictions=tf.argmax(self.outputs, axis=1))\n",
    "    \n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer()) # don't forget to initial all variables\n",
    "        self.saver = tf.train.Saver() # a saver is for saving or restoring your trained weight\n",
    "        \n",
    "    def train(self, batch_x, batch_y):\n",
    "        fd = {}\n",
    "        fd[self.inputs] = batch_x\n",
    "        fd[self.groundtruths] = batch_y\n",
    "        # feed in input and groundtruth to get loss and update the weight via Adam optimizer\n",
    "        loss, accuracy, _ = self.sess.run([self.loss, self.accuracy, self.optimizer], fd)\n",
    "        \n",
    "        return loss, accuracy\n",
    "    \n",
    "    def test(self, batch_x, batch_y):\n",
    "        fd = {}\n",
    "        fd[self.inputs] = batch_x\n",
    "        fd[self.groundtruths] = batch_y\n",
    "        prediction, accuracy = self.sess.run([self.outputs, self.accuracy], fd)\n",
    "            \n",
    "        return prediction, accuracy\n",
    "    \n",
    "    def save(self, e):\n",
    "        self.saver.save(self.sess, 'model/rnn/rnn_%d.ckpt'%(e+1))\n",
    "    \n",
    "    def restore(self, e):\n",
    "        self.saver.restore(self.sess, 'model/rnn/rnn_%d.ckpt'%(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameter of our network\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_batch = BatchGenerator(train_dat, BATCH_SIZE)\n",
    "test_batch = BatchGenerator(test_dat, BATCH_SIZE)\n",
    "\n",
    "n_train = len(train_dat)//BATCH_SIZE\n",
    "n_test = len(test_dat)//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model = SentimentReviewRNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preparing batch and model, we can train it and see how the performance is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rec_loss = []\n",
    "\n",
    "for e in range(EPOCHS): # train for several epochs\n",
    "    loss_train = 0\n",
    "    accuracy_train = 0\n",
    "    \n",
    "    for b in range(n_train): # feed batches one by one\n",
    "        batch_x, batch_y, _ = train_batch.get(b)\n",
    "        loss_batch, accuracy_batch = model.train(batch_x, batch_y)\n",
    "        \n",
    "        loss_train += loss_batch\n",
    "        accuracy_train += accuracy_batch\n",
    "    \n",
    "    loss_train /= n_train\n",
    "    accuracy_train /= n_train\n",
    "    \n",
    "    model.save(e) # save your model after each epoch\n",
    "    rec_loss.append([loss_train, accuracy_train])\n",
    "\n",
    "np.save('./model/rnn/rec_loss.npy', rec_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss curve and accuracy curve are both pretty good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_loss = np.load('./model/rnn/rec_loss.npy')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt_loss = plt.plot([rec_loss[i][0] for i in range(len(rec_loss))])\n",
    "plt_accuracy = plt.plot([rec_loss[i][1] for i in range(len(rec_loss))])\n",
    "plt.legend(['Loss', 'Accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing, we can **just restore** trained weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.restore(EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_test = 0\n",
    "\n",
    "for b in range(n_test):\n",
    "    batch_x, batch_y, _ = test_batch.get(b)\n",
    "    _, accuracy_batch = model.test(batch_x, batch_y)\n",
    "        \n",
    "    accuracy_test += accuracy_batch\n",
    "    \n",
    "accuracy_test /= n_test\n",
    "\n",
    "print('Test: %.4f'%(accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset -sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Sequence-to-Sequence</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/seq2seq.png' width='60%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequence-to-Sequence (Seq2Seq)** models have enjoyed great success in a variety of tasks such as machine translation, speech recognition, and text summarization. Seq2Seq is constructed of two RNNs, **encoder and decoder**. Encoder encodes input sequence into a hidden vector first, and decoder will gererate words one by one based on it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Neural Machine Translation</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Machine Translation (NMT)** is one of NLP tasks, which uses deep learning to solve language translation. Here we use [UM-Corpus](http://www.lrec-conf.org/proceedings/lrec2014/pdf/774_Paper.pdf) and typical Seq2Seq with **attention mechanism** to build our NMT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at dataset (TA has preprocessed first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "en_corpus = np.load('./dataset/translate/enCorpus.npy')\n",
    "en_vocab = np.load('./dataset/translate/enVocab.npy').tolist() # use tolist() to transform back to dict()\n",
    "en_rev = np.load('./dataset/translate/enRev.npy').tolist()\n",
    "\n",
    "ch_corpus = np.load('./dataset/translate/chCorpus.npy')\n",
    "ch_vocab = np.load('./dataset/translate/chVocab.npy').tolist()\n",
    "ch_rev = np.load('./dataset/translate/chRev.npy').tolist()\n",
    "\n",
    "for i in range(4):\n",
    "    print(' '.join([en_rev[en_corpus[i][j]]  for j in range(len(en_corpus[i]))]))\n",
    "    print(' '.join([ch_rev[ch_corpus[i][j]]  for j in range(len(ch_corpus[i]))]))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the presentation issue, we only preserve sentences without 'UNK' token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_corpus_clean = []\n",
    "ch_corpus_clean = []\n",
    "\n",
    "for i in range(len(en_corpus)):\n",
    "    if not(en_vocab['<UNK>'] in en_corpus[i] or ch_vocab['<UNK>'] in ch_corpus[i]): # remove '<UNK>' sentence\n",
    "        en_corpus_clean.append(en_corpus[i])\n",
    "        ch_corpus_clean.append(ch_corpus[i])\n",
    "\n",
    "for i in range(4):\n",
    "    print(' '.join([en_rev[en_corpus_clean[i][j]]  for j in range(len(en_corpus_clean[i]))]))\n",
    "    print(' '.join([ch_rev[ch_corpus_clean[i][j]]  for j in range(len(ch_corpus_clean[i]))]))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Translation Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we use [legacy_seq2seq](https://www.tensorflow.org/api_docs/python/tf/contrib/legacy_seq2seq/embedding_attention_seq2seq) which provides a fasy way to build up seq2seq model with attention. Be careful that legacy_seq2seq is **time major** which means the **input and output should be a list** contains word batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/machine_translation.png' width='60%' />\n",
    "<img src='imgs/time_major.png' width='60%' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_max_len = 0\n",
    "ch_max_len = 0\n",
    "\n",
    "for i in range(len(en_corpus_clean)): # caculate max length\n",
    "    en_max_len = max(en_max_len, len(en_corpus_clean[i]))\n",
    "    ch_max_len = max(ch_max_len, len(ch_corpus_clean[i]))\n",
    "\n",
    "print(en_max_len, ch_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator:\n",
    "    def __init__(self, en_corpus, ch_corpus, en_pad, ch_pad, en_max_len, ch_max_len, batch_size):\n",
    "        n = len(en_corpus)\n",
    "        batch_num = len(en_corpus)//batch_size\n",
    "        n = batch_num*batch_size\n",
    "        \n",
    "        self.xs = [np.zeros(n, dtype=np.int32) for _ in range(en_max_len)] # encoder inputs\n",
    "        self.ys = [np.zeros(n, dtype=np.int32) for _ in range(ch_max_len)] # decoder inputs\n",
    "        self.gs = [np.zeros(n, dtype=np.int32) for _ in range(ch_max_len)] # decoder outputs\n",
    "        self.ws = [np.zeros(n, dtype=np.float32) for _ in range(ch_max_len)] # decoder weight for loss caculation\n",
    "        \n",
    "        self.en_max_len = en_max_len\n",
    "        self.ch_max_len = ch_max_len\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        for b in range(batch_num):\n",
    "            for i in range(b*batch_size, (b+1)*batch_size):\n",
    "                for j in range(len(en_corpus[i])-2):\n",
    "                    self.xs[j][i] = en_corpus[i][j+1]\n",
    "                for j in range(j+1, en_max_len):\n",
    "                    self.xs[j][i] = en_pad\n",
    "                \n",
    "                for j in range(len(ch_corpus[i])-1):\n",
    "                    self.ys[j][i] = ch_corpus[i][j]\n",
    "                    self.gs[j][i] = ch_corpus[i][j+1]\n",
    "                    self.ws[j][i] = 1.0\n",
    "                for j in range(j+1, ch_max_len): # don't forget padding and let loss weight zero\n",
    "                    self.ys[j][i] = ch_pad\n",
    "                    self.gs[j][i] = ch_pad\n",
    "                    self.ws[j][i] = 0.0\n",
    "    \n",
    "    def get(self, batch_id):\n",
    "        x = [self.xs[i][batch_id*self.batch_size:(batch_id+1)*self.batch_size] for i in range(self.en_max_len)]\n",
    "        y = [self.ys[i][batch_id*self.batch_size:(batch_id+1)*self.batch_size] for i in range(self.ch_max_len)]\n",
    "        g = [self.gs[i][batch_id*self.batch_size:(batch_id+1)*self.batch_size] for i in range(self.ch_max_len)]\n",
    "        w = [self.ws[i][batch_id*self.batch_size:(batch_id+1)*self.batch_size] for i in range(self.ch_max_len)]\n",
    "        \n",
    "        return x, y, g, w\n",
    "\n",
    "batch = BatchGenerator(en_corpus_clean, ch_corpus_clean, \n",
    "                       en_vocab['<PAD>'], ch_vocab['<PAD>'], en_max_len, ch_max_len, 4)\n",
    "\n",
    "x, y, g, w = batch.get(2)\n",
    "for i in range(4):\n",
    "    print(' '.join([en_rev[x[j][i]] for j in range(en_max_len)]))\n",
    "    print(' '.join([ch_rev[y[j][i]] for j in range(ch_max_len)]))\n",
    "    print(' '.join([ch_rev[g[j][i]] for j in range(ch_max_len)]))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Seq2Seq Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/attention.jpg' width='60%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training Seq2Seq, we usually use a trick named **teacher forcing** which can help train more efficiently. But when testing, there isn't teacher any more. In tensorflow implementation, we need to **build 2 same models** with one feeding previous. Since they both **share same weight**, don't forget **reuse RNN cell and model in variable scope**. **Attention mechanim** let decoder **focus on specific input** when decding, and generate more accurate output. Thanks to legacy_seq2seq, attention has been implemented also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MachineTranslationSeq2Seq:\n",
    "    def __init__(self, en_max_len, ch_max_len, en_size, ch_size):\n",
    "        self.en_max_len = en_max_len\n",
    "        self.ch_max_len = ch_max_len\n",
    "        \n",
    "        with tf.variable_scope('seq2seq_intput/output'):\n",
    "            self.enc_inputs = [tf.placeholder(tf.int32, [None]) for i in range(en_max_len)] # time mojor feed\n",
    "            self.dec_inputs = [tf.placeholder(tf.int32, [None]) for i in range(ch_max_len)]\n",
    "            self.groundtruths = [tf.placeholder(tf.int32, [None]) for i in range(ch_max_len)]\n",
    "            self.weights = [tf.placeholder(tf.float32, [None]) for i in range(ch_max_len)]\n",
    "            \n",
    "        with tf.variable_scope('seq2seq_rnn'): # training by teacher forcing\n",
    "            self.out_cell = tf.contrib.rnn.LSTMCell(512)\n",
    "            self.outputs, _ = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(self.enc_inputs, self.dec_inputs, \n",
    "                                                                                    self.out_cell, \n",
    "                                                                                    en_size, ch_size, 300)\n",
    "        with tf.variable_scope('seq2seq_rnn', reuse=True): # predict by feeding previous\n",
    "            self.pred_cell = tf.contrib.rnn.LSTMCell(512, reuse=True) # reuse cell for train and test\n",
    "            self.predictions, _ = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(self.enc_inputs, self.dec_inputs, \n",
    "                                                                                        self.pred_cell, \n",
    "                                                                                        en_size, ch_size, 300, \n",
    "                                                                                        feed_previous=True)\n",
    "        \n",
    "        with tf.variable_scope('loss'):\n",
    "            # caculate weighted loss\n",
    "            self.loss = tf.reduce_mean(tf.contrib.legacy_seq2seq.sequence_loss_by_example(self.outputs, \n",
    "                                                                                          self.groundtruths, \n",
    "                                                                                          self.weights))\n",
    "            self.optimizer = tf.train.AdamOptimizer(0.002).minimize(self.loss)\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def train(self, x, y, g, w):\n",
    "        fd = {}\n",
    "        for i in range(self.en_max_len):\n",
    "            fd[self.enc_inputs[i]] = x[i] # show how to feed a list\n",
    "        \n",
    "        for i in range(self.ch_max_len):\n",
    "            fd[self.dec_inputs[i]] = y[i]\n",
    "            fd[self.groundtruths[i]] = g[i]\n",
    "            fd[self.weights[i]] = w[i]\n",
    "        \n",
    "        loss, _ = self.sess.run([self.loss, self.optimizer], fd)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def output(self, x, y):\n",
    "        fd = {}\n",
    "        for i in range(self.en_max_len):\n",
    "            fd[self.enc_inputs[i]] = x[i]\n",
    "        \n",
    "        for i in range(self.ch_max_len):\n",
    "            fd[self.dec_inputs[i]] = y[i]\n",
    "        \n",
    "        out = self.sess.run(self.outputs, fd)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def predict(self, x, ch_beg):\n",
    "        fd = {}\n",
    "        for i in range(self.en_max_len):\n",
    "            fd[self.enc_inputs[i]] = x[i]\n",
    "        \n",
    "        for i in range(self.ch_max_len): # when feed previous, the fist token should be '<BEG>', and others are useless\n",
    "            if i==0:\n",
    "                fd[self.dec_inputs[i]] = np.ones(y[i].shape, dtype=np.int32)*ch_beg\n",
    "            else:\n",
    "                fd[self.dec_inputs[i]] = np.zeros(y[i].shape, dtype=np.int32)\n",
    "        \n",
    "        pd = self.sess.run(self.predictions, fd)\n",
    "        \n",
    "        return pd\n",
    "    \n",
    "    def save(self, e):\n",
    "        self.saver.save(self.sess, 'model/seq2seq/seq2seq_%d.ckpt'%(e+1))\n",
    "    \n",
    "    def restore(self, e):\n",
    "        self.saver.restore(self.sess, 'model/seq2seq/seq2seq_%d.ckpt'%(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model = MachineTranslationSeq2Seq(en_max_len, ch_max_len, len(en_vocab), len(ch_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 40\n",
    "BATCH_SIZE = 256\n",
    "batch_num = len(en_corpus_clean)//BATCH_SIZE\n",
    "\n",
    "batch = BatchGenerator(en_corpus_clean, ch_corpus_clean, \n",
    "                       en_vocab['<PAD>'], ch_vocab['<PAD>'], \n",
    "                       en_max_len, ch_max_len, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rec_loss = []\n",
    "for e in range(EPOCHS):\n",
    "    train_loss = 0\n",
    "    \n",
    "    for b in range(batch_num):\n",
    "        x, y, g, w = batch.get(b)\n",
    "        batch_loss = model.train(x, y, g, w)\n",
    "        train_loss += batch_loss\n",
    "    \n",
    "    train_loss /= batch_num\n",
    "    rec_loss.append(train_loss)\n",
    "    model.save(e)\n",
    "    \n",
    "np.save('./model/seq2seq/rec_loss.npy', rec_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss curve is also good this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_loss = np.load('./model/seq2seq/rec_loss.npy')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt_loss = plt.plot([rec_loss[i] for i in range(len(rec_loss))])\n",
    "plt.legend(['Loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.restore(EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cherry Pick and Show Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/bleu.png' width='60%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[BLEU](https://en.wikipedia.org/wiki/BLEU) is a metric for supervised text generation which finds the **similarity between two sentence based on n-gram token**. Now, we want to show some great translation result which is called **cherry pick**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def cherry_pick(records, n, upper_bound=1.0):\n",
    "    bleus = []\n",
    "    \n",
    "    for en, ch_gr, ch_pd in records:\n",
    "        bleu = nltk.translate.bleu_score.sentence_bleu([ch_gr], ch_pd) # caculate BLEU by nltk\n",
    "        bleus.append(bleu)\n",
    "    \n",
    "    lst = [i for i in range(len(records)) if bleus[i]<=upper_bound]\n",
    "    lst = sorted(lst, key=lambda i: bleus[i], reverse=True) # sort by BLEU score\n",
    "    \n",
    "    return [records[lst[i]] for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd\n",
    "\n",
    "records = []\n",
    "\n",
    "for i in range(10):\n",
    "    i = rd.randint(0, batch_num-1) # random pick one to translate\n",
    "    \n",
    "    x, y, g, w = batch.get(i)\n",
    "    out = model.output(x, y)\n",
    "    pd = model.predict(x, ch_vocab['<BEG>'])\n",
    "\n",
    "    for j in range(10):\n",
    "        j = rd.randint(0, BATCH_SIZE-1)\n",
    "        \n",
    "        en = [en_rev[x[i][j]] for i in range(en_max_len)]\n",
    "        en = en[:en.index('<PAD>')]\n",
    "        ch_gr = [ch_rev[g[i][j]] for i in range(ch_max_len)]\n",
    "        if '<END>' in ch_gr:\n",
    "            ch_gr = ch_gr[:ch_gr.index('<END>')]\n",
    "        ch_pd = [ch_rev[np.argmax(pd[i][j, :])] for i in range(ch_max_len)]\n",
    "        if '<END>' in ch_pd:\n",
    "            ch_pd = ch_pd[:ch_pd.index('<END>')]\n",
    "        \n",
    "        records.append([en, ch_gr, ch_pd])\n",
    "\n",
    "n = 12 # how many result we show\n",
    "rec_cherry = cherry_pick(records, n)\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(3):\n",
    "        print(' '.join(rec_cherry[i][j]))\n",
    "    \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset -sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you have to train a **ChatBot** using Seq2Seq model and [Cornell Movie-Dialogs](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html). All you have to do includes **text preprocessing**, **batch preparation**, **model training**, and **cherry pick**. You can download all dataset and trained model in this lab [here]()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notification:\n",
    "+ Submit on iLMS your code file (ex: Lab14-103062110.ipynb)\n",
    "+ Give a **brief report** for every parts you have done\n",
    "+ The deadline will be **2017/12/07 23:59**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
