{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Done\n"
     ]
    }
   ],
   "source": [
    "from lab12_util import *\n",
    "\n",
    "DATA_URL = 'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'\n",
    "DEST_DIRECTORY = 'dataset/cifar10'\n",
    "DATA_DIRECTORY = DEST_DIRECTORY + '/cifar-10-batches-bin'\n",
    "IMAGE_HEIGHT = 32\n",
    "IMAGE_WIDTH = 32\n",
    "IMAGE_DEPTH = 3\n",
    "IMAGE_SIZE_CROPPED = 24\n",
    "BATCH_SIZE = 128\n",
    "NUM_CLASSES = 10 \n",
    "LABEL_BYTES = 1\n",
    "IMAGE_BYTES = 32 * 32 * 3\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 10000\n",
    "\n",
    "# download it\n",
    "maybe_download_and_extract(DEST_DIRECTORY, DATA_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.data import FixedLengthRecordDataset, Iterator\n",
    "\n",
    "def cifar10_record_distort_parser(record):\n",
    "    ''' Parse the record into label, cropped and distorted image\n",
    "    -----\n",
    "    Args:\n",
    "        record: \n",
    "            a record containing label and image.\n",
    "    Returns:\n",
    "        label: \n",
    "            the label in the record.\n",
    "        image: \n",
    "            the cropped and distorted image in the record.\n",
    "    '''\n",
    "    label_bytes = 1\n",
    "    image_bytes = IMAGE_HEIGHT * IMAGE_WIDTH * IMAGE_DEPTH\n",
    "    record_bytes = label_bytes + image_bytes\n",
    "\n",
    "    # Bytes to Vector\n",
    "    record_vector = tf.decode_raw(record, tf.uint8)\n",
    "\n",
    "    label = tf.cast(record_vector[0], tf.int32)\n",
    "    #label = tf.one_hot(label, NUM_CLASSES, tf.int32)\n",
    "\n",
    "    depth_major = tf.reshape(record_vector[label_bytes:record_bytes], [IMAGE_DEPTH, IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "\n",
    "    reshaped_image = tf.cast(tf.transpose(depth_major, [1, 2, 0]), tf.float32)\n",
    "    distorted_image = tf.random_crop(reshaped_image, [IMAGE_SIZE_CROPPED, IMAGE_SIZE_CROPPED, 3])\n",
    "    distorted_image = tf.image.random_flip_left_right(distorted_image)\n",
    "    distorted_image = tf.image.random_brightness(distorted_image, max_delta=63)\n",
    "    \n",
    "    image = tf.image.per_image_standardization(distorted_image)\n",
    "    \n",
    "    \n",
    "    return label, image\n",
    "    \n",
    "\n",
    "\n",
    "def cifar10_record_crop_parser(record):\n",
    "    ''' Parse the record into label, cropped image\n",
    "    -----\n",
    "    Args:\n",
    "        record: \n",
    "            a record containing label and image.\n",
    "    Returns:\n",
    "        label: \n",
    "            the label in the record.\n",
    "        image: \n",
    "            the cropped image in the record.\n",
    "    '''\n",
    "    label_bytes = 1\n",
    "    image_bytes = IMAGE_HEIGHT * IMAGE_WIDTH * IMAGE_DEPTH\n",
    "    record_bytes = label_bytes + image_bytes\n",
    "    \n",
    "    # Bytes to Vector\n",
    "    record_vector = tf.decode_raw(record, tf.uint8)\n",
    "    \n",
    "    label = tf.cast(record_vector[0], tf.int32)\n",
    "    #label = tf.one_hot(label, NUM_CLASSES, tf.int32)\n",
    "    \n",
    "    depth_major = tf.reshape(\n",
    "      record_vector[label_bytes:record_bytes], [IMAGE_DEPTH, IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    \n",
    "    reshaped_image = tf.cast(tf.transpose(depth_major, [1, 2, 0]), tf.float32)\n",
    "    distorted_image = tf.random_crop(reshaped_image, [IMAGE_SIZE_CROPPED, IMAGE_SIZE_CROPPED, 3])\n",
    "    \n",
    "    image = tf.image.per_image_standardization(distorted_image)\n",
    "    \n",
    "    \n",
    "    return label, image\n",
    "\n",
    "\n",
    "def cifar10_iterator(filenames, batch_size, cifar10_record_parser):\n",
    "    ''' Create a dataset and return a tf.contrib.data.Iterator \n",
    "    which provides a way to extract elements from this dataset.\n",
    "    -----\n",
    "    Args:\n",
    "        filenames: \n",
    "            a tensor of filenames.\n",
    "        batch_size: \n",
    "            batch size.\n",
    "    Returns:\n",
    "        iterator: \n",
    "            an Iterator providing a way to extract elements from the created dataset.\n",
    "        output_types: \n",
    "            the output types of the created dataset.\n",
    "        output_shapes: \n",
    "            the output shapes of the created dataset.\n",
    "    '''\n",
    "    label_bytes = 1\n",
    "    image_bytes = IMAGE_HEIGHT * IMAGE_WIDTH * IMAGE_DEPTH\n",
    "    record_bytes = label_bytes + image_bytes\n",
    "    dataset = FixedLengthRecordDataset(filenames, record_bytes)\n",
    "    \n",
    "    dataset = dataset.map(cifar10_record_parser)\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.repeat(10)\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    \n",
    "    output_types = dataset.output_types\n",
    "    output_shapes = dataset.output_shapes\n",
    "\n",
    "\n",
    "    return iterator, output_types, output_shapes\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "training_files = [os.path.join(DATA_DIRECTORY, 'data_batch_%d.bin' % i) for i in range(1, 6)]\n",
    "testing_files = [os.path.join(DATA_DIRECTORY, 'test_batch.bin')]\n",
    "\n",
    "filenames_train = tf.constant(training_files)\n",
    "filenames_test = tf.constant(testing_files)\n",
    "\n",
    "iterator_train, types, shapes = cifar10_iterator(filenames_train, BATCH_SIZE, cifar10_record_distort_parser)\n",
    "iterator_test, _, _ = cifar10_iterator(filenames_test, BATCH_SIZE, cifar10_record_crop_parser)\n",
    "\n",
    "next_batch = iterator_train.get_next()\n",
    "\n",
    "# use to handle training and testing\n",
    "handle = tf.placeholder(tf.string, shape=[])\n",
    "iterator = Iterator.from_string_handle(handle, types, shapes)\n",
    "labels_images_pairs = iterator.get_next()\n",
    "\n",
    "# CNN model\n",
    "model = CNN_Model(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_training_example=NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN,\n",
    "    num_epoch_per_decay=350.0,\n",
    "    init_lr=0.1,\n",
    "    moving_average_decay=0.9999)\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    labels, images = labels_images_pairs\n",
    "    labels = tf.reshape(labels, [BATCH_SIZE])\n",
    "    images = tf.reshape(images, [BATCH_SIZE, IMAGE_SIZE_CROPPED, IMAGE_SIZE_CROPPED, IMAGE_DEPTH])\n",
    "with tf.variable_scope('model'):\n",
    "    logits = model.inference(images)\n",
    "\n",
    "# train\n",
    "global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "total_loss = model.loss(logits, labels)\n",
    "train_op = model.train(total_loss, global_step)\n",
    "\n",
    "# test\n",
    "top_k_op = tf.nn.in_top_k(logits, labels, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-12 15:31:28.381171: Start training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 1/10 [00:11<01:45, 11.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-12 15:31:40.012861: Loss of epoch 1.0: 1504.51416015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [00:24<01:36, 12.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-12 15:31:52.367995: Loss of epoch 2.0: 1173.6761474609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [00:35<01:22, 11.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-12 15:32:03.479606: Loss of epoch 3.0: 955.1044921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [00:46<01:09, 11.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-12 15:32:14.608683: Loss of epoch 4.0: 796.3057861328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [00:57<00:57, 11.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-12 15:32:25.993432: Loss of epoch 5.0: 682.933837890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [01:09<00:46, 11.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-12 15:32:37.419351: Loss of epoch 6.0: 599.81005859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [01:20<00:34, 11.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-12 15:32:48.691900: Loss of epoch 7.0: 540.1226806640625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [01:32<00:23, 11.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-12 15:33:00.474425: Loss of epoch 8.0: 492.43048095703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [01:43<00:11, 11.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-12 15:33:12.198794: Loss of epoch 9.0: 459.7913818359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 10/10 [01:55<00:00, 11.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-12 15:33:23.625325: Loss of epoch 10.0: 432.41925048828125\n",
      "2017-11-12 15:33:23.749019: Done training.\n",
      "CPU times: user 3min 29s, sys: 17.5 s, total: 3min 47s\n",
      "Wall time: 1min 55s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "NUM_EPOCH = 10\n",
    "NUM_BATCH_PER_EPOCH = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN // BATCH_SIZE\n",
    "ckpt_dir = './model/'\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement=True, gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "\n",
    "# train\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session(config=config) as sess:\n",
    "    ckpt = tf.train.get_checkpoint_state(ckpt_dir)\n",
    "    \n",
    "    if (ckpt and ckpt.model_checkpoint_path):\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        # assume the name of checkpoint is like '.../model.ckpt-1000'\n",
    "        gs = int(ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1])\n",
    "        sess.run(tf.assign(global_step, gs))\n",
    "    else:\n",
    "        # no checkpoint found\n",
    "        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "        sess.run(init_op)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        loss = []\n",
    "        \n",
    "        print(\"{}: Start training.\".format(datetime.now()))\n",
    "        \n",
    "    for i in tqdm(range(NUM_EPOCH)):\n",
    "        _loss = []\n",
    "        sess.run(iterator_train.initializer)\n",
    "\n",
    "        for _ in range(NUM_BATCH_PER_EPOCH):\n",
    "            lbl, img = sess.run(next_batch)\n",
    "            l, _ = sess.run([total_loss, train_op], feed_dict={images: img, labels: lbl})\n",
    "            _loss.append(l)\n",
    "        loss_this_epoch = np.sum(_loss)\n",
    "        gs = global_step.eval()\n",
    "        print('{}: Loss of epoch {}: {}'.format(datetime.now(), gs / NUM_BATCH_PER_EPOCH, loss_this_epoch))\n",
    "        loss.append(loss_this_epoch)\n",
    "        saver.save(sess, ckpt_dir + 'model.ckpt', global_step=gs)\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "  \n",
    "print(\"{}: Done training.\".format(datetime.now()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/model.ckpt-3900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 50.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-12 15:41:53.104205: Accurarcy: 7255/9984 = 0.7266626602564102\n",
      "CPU times: user 3.19 s, sys: 244 ms, total: 3.43 s\n",
      "Wall time: 1.72 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "next_test = iterator_test.get_next()\n",
    "variables_to_restore = model.ema.variables_to_restore()\n",
    "saver = tf.train.Saver(variables_to_restore)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Restore variables from disk.\n",
    "    ckpt = tf.train.get_checkpoint_state(ckpt_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        num_iter = NUM_EXAMPLES_PER_EPOCH_FOR_EVAL // BATCH_SIZE\n",
    "        total_sample_count = num_iter * BATCH_SIZE\n",
    "        true_count = 0\n",
    "        sess.run(iterator_test.initializer)\n",
    "        for _ in tqdm(range(num_iter)):\n",
    "            lbl, img = sess.run(next_test)\n",
    "            predictions = sess.run(top_k_op, feed_dict={images: img, labels: lbl})\n",
    "            true_count += np.sum(predictions)\n",
    "        print('{}: Accurarcy: {}/{} = {}'.format(datetime.now(), true_count, total_sample_count,\n",
    "                                     true_count / total_sample_count))\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "    else:\n",
    "        print(\"{}: No model existed.\".format(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
